{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292e96df",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for Predicting Stock Market Movements Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a02b6a",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7ce332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff00ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning\n",
    "import contractions\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1664c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\deiro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text pre-procesing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#PoS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882530b",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e70aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/Combined_News_DJIA.csv', encoding = \"ISO-8859-1\", parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6934ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start and end date\n",
    "start_date = '2008-07-15'\n",
    "end_date = '2016-07-02'\n",
    "tkr_djia ='^DJI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afdfd3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "DJIA = yf.download(tkr_djia, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2213d",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617de1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_djia=pd.DataFrame(DJIA)\n",
    "df_djia = df_djia.reset_index()\n",
    "df_djia = df_djia.sort_values(by=['Date'], ascending=False,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c509efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_djia['Date'] = df_djia['Date'].dt.date\n",
    "df_djia['Date'] = pd.to_datetime(df_djia['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3bc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_djia['Next_1_Adj_Close'] = df_djia['Adj Close'].shift(-1)\n",
    "df_djia['Next_2_Adj_Close'] = df_djia['Adj Close'].shift(-2)\n",
    "df_djia['Next_3_Adj_Close'] = df_djia['Adj Close'].shift(-3)\n",
    "df_djia['Next_4_Adj_Close'] = df_djia['Adj Close'].shift(-4)\n",
    "df_djia['Next_5_Adj_Close'] = df_djia['Adj Close'].shift(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10381a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_djia['Label_1day'] = np.where(df_djia['Next_1_Adj_Close'] >= df_djia['Adj Close'], 1, 0)\n",
    "df_djia['Label_2day'] = np.where(df_djia['Next_2_Adj_Close'] >= df_djia['Adj Close'], 1, 0)\n",
    "df_djia['Label_3day'] = np.where(df_djia['Next_3_Adj_Close'] >= df_djia['Adj Close'], 1, 0)\n",
    "df_djia['Label_4day'] = np.where(df_djia['Next_4_Adj_Close'] >= df_djia['Adj Close'], 1, 0)\n",
    "df_djia['Label_5day'] = np.where(df_djia['Next_5_Adj_Close'] >= df_djia['Adj Close'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ddfa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_djia.to_csv('dataset/upload_DJIA_table.csv',sep=',', encoding='utf-8',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec0460",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1687e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, 'no news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7511476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that contains all the headlines from Top0 to Top25\n",
    "df[\"news\"] = df.filter(regex=(\"Top.*\")).apply(lambda x: ''.join(str(x.values)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1805f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the name of the headline columns\n",
    "cols = []\n",
    "for i in range(1,26):\n",
    "    col = (\"Top{}\".format(i))\n",
    "    cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf6157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_cleaning(text):\n",
    "    # Remove the HTML tags    \n",
    "    text = re.sub('b\\\"|b\\'|\\\\\\\\|\\\\\\\"', '', text)\n",
    "    # Remove non ASCII\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    # Remove any punctuation\n",
    "    text = text.translate(text.maketrans('', '', string.punctuation))\n",
    "    # Remove any extra whitespace    \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # Change US to usa (in this way it is not confused with the pronoun us)\n",
    "    text = re.sub(r'US', 'usa', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower() \n",
    "    # Chage to the abbrevation    \n",
    "    text = re.sub(r\"united states of america\", \"usa\", text)\n",
    "    # Chage to the abbrevation \n",
    "    text = re.sub(r\"america\", \"usa\", text)\n",
    "    # Remove contractions \n",
    "    text = contractions.fix(text)\n",
    "    #Remove possessive noun\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    # Remove any HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)  \n",
    "    # Remove numbers \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove any special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a455a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_news'] = df['news'].apply(lambda x: txt_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c17d7eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cols):\n",
    "    df[col] = df[col].apply(lambda x: txt_cleaning(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b758245",
   "metadata": {},
   "source": [
    "### Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f06602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deiro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deiro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d786af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "df['tokenized'] = df['clean_news'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec2f2610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:12<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cols):\n",
    "    df[col] = df[col].apply(lambda x: word_tokenize(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9e70cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize stopwords removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['news_without_stopwords'] = df['tokenized'].apply(lambda words: [word for word in words if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64436758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 70.42it/s] \n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cols):\n",
    "    df[col] = df[col].apply(lambda words: [word for word in words if word not in stop_words]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3043f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['news_stemmed'] = df['news_without_stopwords'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44fa9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PoS\n",
    "df['news_pos'] = df['news_without_stopwords'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "362c20e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:39<00:00,  4.00s/it]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cols):\n",
    "    df[col] = df[col].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c085e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma without PoS\n",
    "lem = WordNetLemmatizer()\n",
    "df['news_lemmatized'] = df['news_without_stopwords'].apply(lambda words: [lem.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8ff1081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1989/1989 [00:05<00:00, 379.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#Lemma with PoS\n",
    "lemma_list = []\n",
    "\n",
    "for words in tqdm(df['news_pos']):\n",
    "    tmp=[]\n",
    "    for lemma, pos in words:\n",
    "        if pos.startswith(\"NN\"):\n",
    "            tmp.append(lem.lemmatize(lemma, pos='n'))\n",
    "        elif pos.startswith('VB'):\n",
    "            tmp.append(lem.lemmatize(lemma, pos='v'))\n",
    "        elif pos.startswith('JJ'):\n",
    "            tmp.append(lem.lemmatize(lemma, pos='a'))\n",
    "        elif pos.startswith('R'):\n",
    "            tmp.append(lem.lemmatize(lemma, pos='r'))\n",
    "        else:\n",
    "            tmp.append(lem.lemmatize(lemma))\n",
    "            \n",
    "    lemma_list.append(tmp)\n",
    "    \n",
    "df['news_lemmatized_pos'] = lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "845388cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cols):\n",
    "    #Lemma with PoS\n",
    "    lemma_list = []\n",
    "\n",
    "    for words in df[col]:\n",
    "        tmp=[]\n",
    "        for lemma, pos in words:\n",
    "            if pos.startswith(\"NN\"):\n",
    "                tmp.append(lem.lemmatize(lemma, pos='n'))\n",
    "            elif pos.startswith('VB'):\n",
    "                tmp.append(lem.lemmatize(lemma, pos='v'))\n",
    "            elif pos.startswith('JJ'):\n",
    "                tmp.append(lem.lemmatize(lemma, pos='a'))\n",
    "            elif pos.startswith('R'):\n",
    "                tmp.append(lem.lemmatize(lemma, pos='r'))\n",
    "            else:\n",
    "                tmp.append(lem.lemmatize(lemma))\n",
    "\n",
    "        lemma_list.append(tmp)\n",
    "\n",
    "    df[col] = lemma_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8333928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>news_without_stopwords</th>\n",
       "      <th>news_stemmed</th>\n",
       "      <th>news_pos</th>\n",
       "      <th>news_lemmatized</th>\n",
       "      <th>news_lemmatized_pos</th>\n",
       "      <th>Label_1day</th>\n",
       "      <th>Label_2day</th>\n",
       "      <th>Label_3day</th>\n",
       "      <th>Label_4day</th>\n",
       "      <th>Label_5day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>[georgia, down, two, russian, warplane, countr...</td>\n",
       "      <td>[break, musharraf, impeach]</td>\n",
       "      <td>[russia, today, columns, troop, roll, south, o...</td>\n",
       "      <td>[russian, tank, move, towards, capital, south,...</td>\n",
       "      <td>[afghan, child, rap, impunity, un, official, s...</td>\n",
       "      <td>[russian, tank, enter, south, ossetia, whilst,...</td>\n",
       "      <td>[break, georgia, invades, south, ossetia, russ...</td>\n",
       "      <td>[enemy, combatent, trial, nothing, sham, salim...</td>\n",
       "      <td>...</td>\n",
       "      <td>[georgia, downs, two, russian, warplanes, coun...</td>\n",
       "      <td>[georgia, down, two, russian, warplan, countri...</td>\n",
       "      <td>[(georgia, JJ), (downs, NNS), (two, CD), (russ...</td>\n",
       "      <td>[georgia, down, two, russian, warplane, countr...</td>\n",
       "      <td>[georgia, down, two, russian, warplane, countr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>[usa, nato, help, u, help, u, help, iraq]</td>\n",
       "      <td>[bush, put, foot, georgian, conflict]</td>\n",
       "      <td>[jewish, georgian, minister, thanks, israeli, ...</td>\n",
       "      <td>[georgian, army, flees, disarray, russian, adv...</td>\n",
       "      <td>[olympic, open, ceremony, firework, fake]</td>\n",
       "      <td>[mossad, fraudulent, new, zealand, passport, i...</td>\n",
       "      <td>[russia, anger, israeli, military, sale, georgia]</td>\n",
       "      <td>[usan, citizen, live, sossetia, blame, usa, ge...</td>\n",
       "      <td>...</td>\n",
       "      <td>[usa, nato, help, us, help, us, help, iraq, bu...</td>\n",
       "      <td>[usa, nato, help, us, help, us, help, iraq, bu...</td>\n",
       "      <td>[(usa, JJ), (nato, NN), (help, NN), (us, PRP),...</td>\n",
       "      <td>[usa, nato, help, u, help, u, help, iraq, bush...</td>\n",
       "      <td>[usa, nato, help, u, help, u, help, iraq, bush...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Label                                               Top1  \\\n",
       "0 2008-08-08      0  [georgia, down, two, russian, warplane, countr...   \n",
       "1 2008-08-11      1          [usa, nato, help, u, help, u, help, iraq]   \n",
       "\n",
       "                                    Top2  \\\n",
       "0            [break, musharraf, impeach]   \n",
       "1  [bush, put, foot, georgian, conflict]   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  [russia, today, columns, troop, roll, south, o...   \n",
       "1  [jewish, georgian, minister, thanks, israeli, ...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  [russian, tank, move, towards, capital, south,...   \n",
       "1  [georgian, army, flees, disarray, russian, adv...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  [afghan, child, rap, impunity, un, official, s...   \n",
       "1          [olympic, open, ceremony, firework, fake]   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  [russian, tank, enter, south, ossetia, whilst,...   \n",
       "1  [mossad, fraudulent, new, zealand, passport, i...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  [break, georgia, invades, south, ossetia, russ...   \n",
       "1  [russia, anger, israeli, military, sale, georgia]   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  [enemy, combatent, trial, nothing, sham, salim...  ...   \n",
       "1  [usan, citizen, live, sossetia, blame, usa, ge...  ...   \n",
       "\n",
       "                              news_without_stopwords  \\\n",
       "0  [georgia, downs, two, russian, warplanes, coun...   \n",
       "1  [usa, nato, help, us, help, us, help, iraq, bu...   \n",
       "\n",
       "                                        news_stemmed  \\\n",
       "0  [georgia, down, two, russian, warplan, countri...   \n",
       "1  [usa, nato, help, us, help, us, help, iraq, bu...   \n",
       "\n",
       "                                            news_pos  \\\n",
       "0  [(georgia, JJ), (downs, NNS), (two, CD), (russ...   \n",
       "1  [(usa, JJ), (nato, NN), (help, NN), (us, PRP),...   \n",
       "\n",
       "                                     news_lemmatized  \\\n",
       "0  [georgia, down, two, russian, warplane, countr...   \n",
       "1  [usa, nato, help, u, help, u, help, iraq, bush...   \n",
       "\n",
       "                                 news_lemmatized_pos Label_1day Label_2day  \\\n",
       "0  [georgia, down, two, russian, warplane, countr...          0          0   \n",
       "1  [usa, nato, help, u, help, u, help, iraq, bush...          0          0   \n",
       "\n",
       "  Label_3day Label_4day Label_5day  \n",
       "0          0          0          0  \n",
       "1          0          0          0  \n",
       "\n",
       "[2 rows x 40 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79c7a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels=df_djia[['Date','Label_1day','Label_2day','Label_3day','Label_4day','Label_5day']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e01aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,df_labels,on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34d2ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pre_process_all_news_days.csv',sep=',', encoding='utf-8',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
